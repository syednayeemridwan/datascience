{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import:\n",
    "- matplotlib.pyplot as plt.\n",
    "- pearsonr from scipy.stats.\n",
    "- Assign column 0 of grains to width and column 1 of grains to length.\n",
    "- Make a scatter plot with width on the x-axis and length on the y-axis.\n",
    "- Use the pearsonr() function to calculate the Pearson correlation of width and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the necessary imports\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# # Assign the 0th column of grains: width\n",
    "# width = grains[:,0]\n",
    "\n",
    "# # Assign the 1st column of grains: length\n",
    "# length = grains[:,1]\n",
    "\n",
    "# # Scatter plot width vs length\n",
    "# plt.scatter(width, length)\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate the Pearson correlation\n",
    "# correlation, pvalue = pearsonr(width, length)\n",
    "\n",
    "# # Display the correlation\n",
    "# print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import PCA from sklearn.decomposition.\n",
    "- Create an instance of PCA called model.\n",
    "- Use the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.\n",
    "- The subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit submit to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Create PCA instance: model\n",
    "# model = PCA()\n",
    "\n",
    "# # Apply the fit_transform method of model to grains: pca_features\n",
    "# pca_features = model.fit_transform(grains)\n",
    "\n",
    "# # Assign 0th column of pca_features: xs\n",
    "# xs = pca_features[:,0]\n",
    "\n",
    "# # Assign 1st column of pca_features: ys\n",
    "# ys = pca_features[:,1]\n",
    "\n",
    "# # Scatter plot xs vs ys\n",
    "# plt.scatter(xs, ys)\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate the Pearson correlation of xs and ys\n",
    "# correlation, pvalue = pearsonr(xs, ys)\n",
    "\n",
    "# # Display the correlation\n",
    "# print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a scatter plot of the grain measurements. This has been done for you.\n",
    "- Create a PCA instance called model.\n",
    "- Fit the model to the grains data.\n",
    "- Extract the coordinates of the mean of the data using the .mean_ attribute of model.\n",
    "- Get the first principal component of model using the .components_[0,:] attribute.\n",
    "- Plot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a scatter plot of the untransformed points\n",
    "# plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# # Create a PCA instance: model\n",
    "# model = PCA()\n",
    "\n",
    "# # Fit model to points\n",
    "# model.fit(grains)\n",
    "\n",
    "# # Get the mean of the grain samples: mean\n",
    "# mean = model.mean_\n",
    "\n",
    "# # Get the first principal component: first_pc\n",
    "# first_pc = model.components_[0,:]\n",
    "\n",
    "# # Plot first_pc as an arrow, starting at mean\n",
    "# plt.arrow(mean[0] , mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# # Keep axes on same scale\n",
    "# plt.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of StandardScaler called scaler.\n",
    "- Create a PCA instance called pca.\n",
    "- Use the make_pipeline() function to create a pipeline chaining scaler and pca.\n",
    "- Use the .fit() method of pipeline to fit it to the fish samples samples.\n",
    "- Extract the number of components used using the .n_components_ attribute of pca. Place this inside a range() function and store the result as features.\n",
    "- Use the plt.bar() function to plot the explained variances, with features on the x-axis and pca.explained_variance_ on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the necessary imports\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create scaler: scaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Create a PCA instance: pca\n",
    "# pca = PCA()\n",
    "\n",
    "# # Create pipeline: pipeline\n",
    "# pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# # Fit the pipeline to 'samples'\n",
    "# pipeline.fit(samples)\n",
    "\n",
    "# # Plot the explained variances\n",
    "# features = range(pca.n_components_)\n",
    "# plt.bar(features, pca.explained_variance_)\n",
    "# plt.xlabel('PCA feature')\n",
    "# plt.ylabel('variance')\n",
    "# plt.xticks(features)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PCA features 0 and 1 have significant variance, the intrinsic dimension of this dataset appears to be 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import PCA from sklearn.decomposition.\n",
    "- Create a PCA instance called pca with n_components=2.\n",
    "- Use the .fit() method of pca to fit it to the scaled fish measurements scaled_samples.\n",
    "- Use the .transform() method of pca to transform the scaled_samples. Assign the result to pca_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Create a PCA model with 2 components: pca\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "# # Fit the PCA instance to the scaled samples\n",
    "# pca.fit(scaled_samples)\n",
    "\n",
    "# # Transform the scaled samples: pca_features\n",
    "# pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# # Print the shape of pca_features\n",
    "# print(pca_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import TfidfVectorizer from sklearn.feature_extraction.text.\n",
    "- Create a TfidfVectorizer instance called tfidf.\n",
    "- Apply .fit_transform() method of tfidf to documents and assign the result to csr_mat. This is a word-frequency array in csr_matrix format.\n",
    "- Inspect csr_mat by calling its .toarray() method and printing the result. This has been done for you.\n",
    "- The columns of the array correspond to words. Get the list of words by calling the .get_feature_names() method of tfidf, and assign the result to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Create a TfidfVectorizer: tfidf\n",
    "# tfidf = TfidfVectorizer() \n",
    "\n",
    "# # Apply fit_transform to document: csr_mat\n",
    "# csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# # Print result of toarray() method\n",
    "# print(csr_mat.toarray())\n",
    "\n",
    "# # Get the words: words\n",
    "# words = tfidf.get_feature_names()\n",
    "\n",
    "# # Print words\n",
    "# print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import:\n",
    "- TruncatedSVD from sklearn.decomposition.\n",
    "- KMeans from sklearn.cluster.\n",
    "- make_pipeline from sklearn.pipeline.\n",
    "- Create a TruncatedSVD instance called svd with n_components=50.\n",
    "- Create a KMeans instance called kmeans with n_clusters=6.\n",
    "- Create a pipeline called pipeline consisting of svd and kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the necessary imports\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# # Create a TruncatedSVD instance: svd\n",
    "# svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# # Create a KMeans instance: kmeans\n",
    "# kmeans = KMeans(n_clusters = 6)\n",
    "\n",
    "# # Create a pipeline: pipeline\n",
    "# pipeline = make_pipeline(svd, kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import pandas as pd.\n",
    "- Fit the pipeline to the word-frequency array articles.\n",
    "- Predict the cluster labels.\n",
    "- Align the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.\n",
    "- Use the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.\n",
    "- Hit submit and take a moment to investigate your amazing clustering of Wikipedia pages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import pandas\n",
    "# import pandas as pd\n",
    "\n",
    "# # Fit the pipeline to articles\n",
    "# pipeline.fit(articles)\n",
    "\n",
    "# # Calculate the cluster labels: labels\n",
    "# labels = pipeline.predict(articles)\n",
    "\n",
    "# # Create a DataFrame aligning labels and titles: df\n",
    "# df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# # Display df sorted by cluster label\n",
    "# print(df.sort_values('label'))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fef59229a6e1ff5434759995be53a28a25493efd6d26eb6c004eefe62e31083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
