{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import pandas as pd.\n",
    "- Read the CSV file 'gapminder.csv' into a DataFrame called df.\n",
    "- Use pandas to create a boxplot showing the variation of life expectancy ('life') by region ('Region'). To do so, pass the column names in to df.boxplot() (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import pandas\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read 'gapminder.csv' into a DataFrame: df\n",
    "# df = pd.read_csv('gapminder.csv')\n",
    "\n",
    "# # Create a boxplot of life expectancy per region\n",
    "# df.boxplot('life', 'Region', rot=60)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the pandas get_dummies() function to create dummy variables from the df DataFrame. Store the result as df_region.\n",
    "- Print the columns of df_region. This has been done for you.\n",
    "- Use the get_dummies() function again, this time specifying drop_first=True to drop the unneeded dummy variable (in this case, 'Region_America').\n",
    "- Hit submit to print the new columns of df_region and take note of how one column was dropped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dummy variables: df_region\n",
    "# df_region = pd.get_dummies(df)\n",
    "\n",
    "# # Print the columns of df_region\n",
    "# print(df_region.columns)\n",
    "\n",
    "# # Create dummy variables with drop_first=True: df_region\n",
    "# df_region = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# # Print the new columns of df_region\n",
    "# print(df_region.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Ridge from sklearn.linear_model and cross_val_score from sklearn.model_selection.\n",
    "- Instantiate a ridge regressor called ridge with alpha=0.5 and normalize=True.\n",
    "- Perform 5-fold cross-validation on X and y using the cross_val_score() function.\n",
    "- Print the cross-validated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary modules\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # Instantiate a ridge regressor: ridge\n",
    "# ridge = Ridge(alpha=0.5 , normalize=True)\n",
    "\n",
    "# # Perform 5-fold cross-validation: ridge_cv\n",
    "# ridge_cv = cross_val_score(ridge, X, y, cv = 5)\n",
    "\n",
    "# # Print the cross-validated scores\n",
    "# print(ridge_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore the DataFrame df in the IPython Shell. Notice how the missing value is represented.\n",
    "- Convert all '?' data points to np.nan.\n",
    "- Count the total number of NaNs using the .isnull() and .sum() methods. This has been done for you.\n",
    "- Drop the rows with missing values from df using .dropna().\n",
    "- Hit submit to see how many rows were lost by dropping the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert '?' to NaN\n",
    "# df[df == '?'] = np.nan\n",
    "\n",
    "# # Print the number of NaNs\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "# # Print shape of original DataFrame\n",
    "# print(\"Shape of Original DataFrame: {}\".format(df.shape))\n",
    "\n",
    "# # Drop missing values and print shape of new DataFrame\n",
    "# df = df.dropna()\n",
    "\n",
    "# # Print shape of new DataFrame\n",
    "# print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Imputer from sklearn.preprocessing and SVC from sklearn.svm. SVC stands for Support Vector Classification, which is a type of SVM.\n",
    "- Setup the Imputation transformer to impute missing data (represented as 'NaN') with the 'most_frequent' value in the column (axis=0).\n",
    "- Instantiate a SVC classifier. Store the result in clf.\n",
    "- Create the steps of the pipeline by creating a list of tuples:\n",
    "- The first tuple should consist of the imputation step, using imp.\n",
    "- The second should consist of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the Imputer module\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Setup the Imputation transformer: imp\n",
    "# imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "\n",
    "# # Instantiate the SVC classifier: clf\n",
    "# clf = SVC()\n",
    "\n",
    "# # Setup the pipeline with the required steps: steps\n",
    "# steps = [('imputation', imp),\n",
    "#         ('SVM', clf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the following modules:\n",
    "- Imputer from sklearn.preprocessing and Pipeline from sklearn.pipeline.\n",
    "- SVC from sklearn.svm.\n",
    "- Create the pipeline using Pipeline() and steps.\n",
    "- Create training and test sets. Use 30% of the data for testing and a random state of 42.\n",
    "- Fit the pipeline to the training set and predict the labels of the test set.\n",
    "- Compute the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary modules\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Setup the pipeline steps: steps\n",
    "# steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "#         ('SVM', SVC())]\n",
    "\n",
    "# # Create the pipeline: pipeline\n",
    "# pipeline = Pipeline(steps)\n",
    "\n",
    "# # Create training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state= 42)\n",
    "\n",
    "# # Fit the pipeline to the train set\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels of the test set\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# # Compute metrics\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import scale from sklearn.preprocessing.\n",
    "- Scale the features X using scale().\n",
    "- Print the mean and standard deviation of the unscaled features X, and then the scaled features X_scaled. Use the numpy functions np.mean() and np.std() to compute the mean and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scale\n",
    "# from sklearn.preprocessing import scale\n",
    "\n",
    "# # Scale the features: X_scaled\n",
    "# X_scaled = scale(X)\n",
    "\n",
    "# # Print the mean and standard deviation of the unscaled features\n",
    "# print(\"Mean of Unscaled Features: {}\".format(np.mean(X) )) \n",
    "# print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n",
    "\n",
    "# # Print the mean and standard deviation of the scaled features\n",
    "# print(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled) )) \n",
    "# print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the following modules:\n",
    "- StandardScaler from sklearn.preprocessing.\n",
    "- Pipeline from sklearn.pipeline.\n",
    "- Complete the steps of the pipeline with StandardScaler() for 'scaler' and KNeighborsClassifier() for 'knn'.\n",
    "- Create the pipeline using Pipeline() and steps.\n",
    "- Create training and test sets, with 30% used for testing. Use a random state of 42.\n",
    "- Fit the pipeline to the training set.\n",
    "- Compute the accuracy scores of the scaled and unscaled models by using the .score() method inside the provided print() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary modules\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Setup the pipeline steps: steps\n",
    "# steps = [('scaler', StandardScaler()),\n",
    "#         ('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# # Create the pipeline: pipeline\n",
    "# pipeline = Pipeline(steps)\n",
    "\n",
    "# # Create train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# # Fit the pipeline to the training set: knn_scaled\n",
    "# knn_scaled = pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Instantiate and fit a k-NN classifier to the unscaled data\n",
    "# knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "# # Compute and print metrics\n",
    "# print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test,y_test)))\n",
    "# print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup the pipeline with the following steps:\n",
    "- Scaling, called 'scaler' with StandardScaler().\n",
    "- Classification, called 'SVM' with SVC().\n",
    "- Specify the hyperparameter space using the following notation: 'step_name__parameter_name'. Here, the step_name is SVM, and the parameter_names are C and gamma.\n",
    "- Create training and test sets, with 20% of the data used for the test set. Use a random state of 21.\n",
    "- Instantiate GridSearchCV with the pipeline and hyperparameter space and fit it to the training set. Use 3-fold cross-validation (This is the default, so you don't have to specify it).\n",
    "- Predict the labels of the test set and compute the metrics. The metrics have been computed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup the pipeline\n",
    "# steps = [('scaler', StandardScaler()),\n",
    "#          ('SVM', SVC())]\n",
    "\n",
    "# pipeline = Pipeline(steps)\n",
    "\n",
    "# # Specify the hyperparameter space\n",
    "# parameters = {'SVM__C':[1, 10, 100],\n",
    "#               'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# # Create train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=21)\n",
    "\n",
    "# # Instantiate the GridSearchCV object: cv\n",
    "# cv = GridSearchCV(pipeline,parameters, cv = 3)\n",
    "\n",
    "# # Fit to the training set\n",
    "# cv.fit( X_train, y_train)\n",
    "\n",
    "# # Predict the labels of the test set: y_pred\n",
    "# y_pred = cv.predict(X_test)\n",
    "\n",
    "# # Compute and print metrics\n",
    "# print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up a pipeline with the following steps:\n",
    "- 'imputation', which uses the Imputer() transformer and the 'mean' strategy to impute missing data ('NaN') using the mean of the column.\n",
    "- 'scaler', which scales the features using StandardScaler().\n",
    "- 'elasticnet', which instantiates an ElasticNet() regressor.\n",
    "- Specify the hyperparameter space for the  ratio using the following notation: 'step_name__parameter_name'. Here, the step_name is elasticnet, and the parameter_name is l1_ratio.\n",
    "- Create training and test sets, with 40% of the data used for the test set. Use a random state of 42.\n",
    "- Instantiate GridSearchCV with the pipeline and hyperparameter space. Use 3-fold cross-validation (This is the default, so you don't have to specify it).\n",
    "- Fit the GridSearchCV object to the training set.\n",
    "- Compute  and the best parameters. This has been done for you, so hit submit to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup the pipeline steps: steps\n",
    "# steps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "#          ('scaler', StandardScaler()),\n",
    "#          ('elasticnet', ElasticNet())]\n",
    "\n",
    "# # Create the pipeline: pipeline \n",
    "# pipeline = Pipeline(steps)\n",
    "\n",
    "# # Specify the hyperparameter space\n",
    "# parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
    "\n",
    "# # Create train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state = 42)\n",
    "\n",
    "# # Create the GridSearchCV object: gm_cv\n",
    "# gm_cv = GridSearchCV(pipeline, parameters, cv = 3)\n",
    "\n",
    "# # Fit to the training set\n",
    "# gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# # Compute and print the metrics\n",
    "# r2 = gm_cv.score(X_test, y_test)\n",
    "# print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
    "# print(\"Tuned ElasticNet R squared: {}\".format(r2))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fef59229a6e1ff5434759995be53a28a25493efd6d26eb6c004eefe62e31083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
