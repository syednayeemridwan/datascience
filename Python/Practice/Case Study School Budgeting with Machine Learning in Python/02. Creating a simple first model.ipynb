{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new DataFrame named numeric_data_only by applying the .fillna(-1000) method to the numeric columns (available in the list NUMERIC_COLUMNS) of df.\n",
    "- Convert the labels (available in the list LABELS) to dummy variables. Save the result as label_dummies.\n",
    "- In the call to multilabel_train_test_split(), set the size of your test set to be 0.2. Use a seed of 123.\n",
    "- Fill in the .info() method calls for X_train, X_test, y_train, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the new DataFrame: numeric_data_only\n",
    "# numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# # Get labels and convert to dummy variables: label_dummies\n",
    "# label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# # Create training and test sets\n",
    "# X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2, seed=123)\n",
    "\n",
    "# # Print the info\n",
    "# print(\"X_train info:\")\n",
    "# print(X_train.info())\n",
    "# print(\"\\nX_test info:\")  \n",
    "# print(X_test.info())\n",
    "# print(\"\\ny_train info:\")  \n",
    "# print(y_train.info())\n",
    "# print(\"\\ny_test info:\")  \n",
    "# print(y_test.info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import LogisticRegression from sklearn.linear_model and OneVsRestClassifier from sklearn.multiclass.\n",
    "- Instantiate the classifier clf by placing LogisticRegression() inside OneVsRestClassifier().\n",
    "- Fit the classifier to the training data X_train and y_train.\n",
    "- Compute and print the accuracy of the classifier using its .score() method, which accepts two arguments: X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import classifiers\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# # Create the DataFrame: numeric_data_only\n",
    "# numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# # Get labels and convert to dummy variables: label_dummies\n",
    "# label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# # Create training and test sets\n",
    "# X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "#                                                                label_dummies,\n",
    "#                                                                size=0.2, \n",
    "#                                                                seed=123)\n",
    "\n",
    "# # Instantiate the classifier: clf\n",
    "# clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# # Fit the classifier to the training data\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Print the accuracy\n",
    "# print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read HoldoutData.csv into a DataFrame called holdout. Specify the keyword argument index_col=0 in your call to read_csv().\n",
    "- Generate predictions using .predict_proba() on the numeric columns (available in the NUMERIC_COLUMNS list) of holdout. Make sure to fill in missing values with -1000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the classifier: clf\n",
    "# clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# # Fit it to the training data\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Load the holdout data: holdout\n",
    "# holdout = pd.read_csv( \"HoldoutData.csv\",index_col=0)\n",
    "\n",
    "# # Generate predictions: predictions\n",
    "# predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the prediction_df DataFrame by specifying the following arguments to the provided parameters pd.DataFrame():\n",
    "- pd.get_dummies(df[LABELS]).columns.\n",
    "- holdout.index.\n",
    "- predictions.\n",
    "- Save prediction_df to a csv file called 'predictions.csv' using the .to_csv() method.\n",
    "- Submit the predictions for scoring by using the score_submission() function with pred_path set to 'predictions.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate predictions: predictions\n",
    "# predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# # Format predictions in DataFrame: prediction_df\n",
    "# prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "#                              index=holdout.index,\n",
    "#                              data=predictions)\n",
    "\n",
    "\n",
    "# # Save prediction_df to csv\n",
    "# prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# # Submit the predictions for scoring: score\n",
    "# score = score_submission(pred_path = 'predictions.csv')\n",
    "\n",
    "# # Print score\n",
    "# print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization = Segmenting Words from Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "- Fill missing values in df.Position_Extra using .fillna('') to replace NaNs with empty strings. Specify the additional keyword argument inplace=True so that you don't have to assign the result back to df.\n",
    "- Instantiate the CountVectorizer as vec_alphanumeric by specifying the token_pattern to be TOKENS_ALPHANUMERIC.\n",
    "- Fit vec_alphanumeric to df.Position_Extra.\n",
    "- Hit submit to see the len of the fitted representation as well as the first 15 elements, and compare to vec_basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# # Fill missing values in df.Position_Extra\n",
    "# df.Position_Extra.fillna('',inplace = True)\n",
    "\n",
    "# # Instantiate the CountVectorizer: vec_alphanumeric\n",
    "# vec_alphanumeric = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# # Fit to the data\n",
    "# vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# # Print the number of tokens and first 15 tokens\n",
    "# msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "# print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "# print(vec_alphanumeric.get_feature_names()[:15])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the .drop() method on data_frame with to_drop and axis= as arguments to drop the non-text data. Save the result as text_data.\n",
    "- Fill in missing values (inplace) in text_data with blanks (\"\"), using the .fillna() method.\n",
    "- Complete the .apply() method by writing a lambda function that uses the .join() method to join all the items in a row with a space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define combine_text_columns()\n",
    "# def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "#     \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "#     # Drop non-text columns that are in the df\n",
    "#     to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "#     text_data = data_frame.drop(to_drop,axis = 1)\n",
    "    \n",
    "#     # Replace nans with blanks\n",
    "#     text_data.fillna('', inplace = True)\n",
    "    \n",
    "#     # Join all text items in a row that have a space in between\n",
    "#     return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "- Instantiate vec_basic and vec_alphanumeric using, respectively, the TOKENS_BASIC and TOKENS_ALPHANUMERIC patterns.\n",
    "- Create the text vector by using the combine_text_columns() function on df.\n",
    "- Using the .fit_transform() method with text_vector, fit and transform first vec_basic and then vec_alphanumeric. Print the number of tokens they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Create the basic token pattern\n",
    "# TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# # Create the alphanumeric token pattern\n",
    "# TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# # Instantiate basic CountVectorizer: vec_basic\n",
    "# vec_basic = CountVectorizer(token_pattern = TOKENS_BASIC)\n",
    "\n",
    "# # Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "# vec_alphanumeric = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# # Create the text vector\n",
    "# text_vector = combine_text_columns(df)\n",
    "\n",
    "# # Fit and transform vec_basic\n",
    "# vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# # Print number of tokens of vec_basic\n",
    "# print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# # Fit and transform vec_alphanumeric\n",
    "# vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# # Print number of tokens of vec_alphanumeric\n",
    "# print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fef59229a6e1ff5434759995be53a28a25493efd6d26eb6c004eefe62e31083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
