{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbols are not counted in tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create text_vector by preprocessing X_train using combine_text_columns. This is important, or else you won't get any tokens!\n",
    "- Instantiate CountVectorizer as text_features. Specify the keyword argument token_pattern=TOKENS_ALPHANUMERIC.\n",
    "- Fit text_features to the text_vector.\n",
    "- Hit submit to print the first 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Create the text vector\n",
    "# text_vector = combine_text_columns(X_train)\n",
    "\n",
    "# # Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# # Instantiate the CountVectorizer: text_features\n",
    "# text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# # Fit text_features to the text vector\n",
    "# text_features.fit(text_vector)\n",
    "\n",
    "# # Print the first 10 tokens\n",
    "# print(text_features.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "- Add a CountVectorizer step to the pipeline with the name 'vectorizer'.\n",
    "- Set the token pattern to be TOKENS_ALPHANUMERIC.\n",
    "- Set the ngram_range to be (1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Import classifiers\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# # Import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Import other preprocessing modules\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# # Select 300 best features\n",
    "# chi_k = 300\n",
    "\n",
    "# # Import functional utilities\n",
    "# from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# # Perform preprocessing\n",
    "# get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "# get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# # Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# # Instantiate pipeline: pl\n",
    "# pl = Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('numeric_features', Pipeline([\n",
    "#                     ('selector', get_numeric_data),\n",
    "#                     ('imputer', Imputer())\n",
    "#                 ])),\n",
    "#                 ('text_features', Pipeline([\n",
    "#                     ('selector', get_text_data),\n",
    "#                     ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "#                                                    ngram_range=(1, 2))),\n",
    "#                     ('dim_red', SelectKBest(chi2, chi_k))\n",
    "#                 ]))\n",
    "#              ]\n",
    "#         )),\n",
    "#         ('scale', MaxAbsScaler()),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An xy term is present, which represents interactions between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the interaction terms step using SparseInteractions() with degree=2. Give it a name of 'int', and make sure it is after the preprocessing step but before scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate pipeline: pl\n",
    "# pl = Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('numeric_features', Pipeline([\n",
    "#                     ('selector', get_numeric_data),\n",
    "#                     ('imputer', Imputer())\n",
    "#                 ])),\n",
    "#                 ('text_features', Pipeline([\n",
    "#                     ('selector', get_text_data),\n",
    "#                     ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "#                                                    ngram_range=(1, 2))),  \n",
    "#                     ('dim_red', SelectKBest(chi2, chi_k))\n",
    "#                 ]))\n",
    "#              ]\n",
    "#         )),\n",
    "#         ('int', SparseInteractions(degree=2)),\n",
    "#         ('scale', MaxAbsScaler()),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video, Peter explained that a hash function takes an input, in your case a token, and outputs a hash value. For example, the input may be a string and the hash value may be an integer.\n",
    "\n",
    "We've loaded a familiar python datatype, a dictionary called hash_dict, that makes this mapping concept a bit more explicit. In fact, python dictionaries ARE hash tables!\n",
    "\n",
    "By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets.\n",
    "\n",
    "Some problems are memory-bound and not easily parallelizable, and hashing enforces a fixed length computation instead of using a mutable datatype (like a dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import HashingVectorizer from sklearn.feature_extraction.text.\n",
    "- Instantiate the HashingVectorizer as hashing_vec using the TOKENS_ALPHANUMERIC pattern.\n",
    "- Fit and transform hashing_vec using text_data. Save the result as hashed_text.\n",
    "- Hit submit to see some of the resulting hash values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import HashingVectorizer\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# # Get text data: text_data\n",
    "# text_data = combine_text_columns(X_train)\n",
    "\n",
    "# # Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "# TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# # Instantiate the HashingVectorizer: hashing_vec\n",
    "# hashing_vec = HashingVectorizer( token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# # Fit and transform the Hashing Vectorizer\n",
    "# hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# # Create DataFrame and print the head\n",
    "# hashed_df = pd.DataFrame(hashed_text.data)\n",
    "# print(hashed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import HashingVectorizer from sklearn.feature_extraction.text.\n",
    "- Add a HashingVectorizer step to the pipeline.\n",
    "- Name the step 'vectorizer'.\n",
    "- Use the TOKENS_ALPHANUMERIC token pattern.\n",
    "- Specify the ngram_range to be (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the hashing vectorizer\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# # Instantiate the winning model pipeline: pl\n",
    "# pl = Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('numeric_features', Pipeline([\n",
    "#                     ('selector', get_numeric_data),\n",
    "#                     ('imputer', Imputer())\n",
    "#                 ])),\n",
    "#                 ('text_features', Pipeline([\n",
    "#                     ('selector', get_text_data),\n",
    "#                     ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "#                                                      non_negative=True, norm=None, binary=False,\n",
    "#                                                      ngram_range=(1, 2))),\n",
    "#                     ('dim_red', SelectKBest(chi2, chi_k))\n",
    "#                 ]))\n",
    "#              ]\n",
    "#         )),\n",
    "#         ('int', SparseInteractions(degree=2)),\n",
    "#         ('scale', MaxAbsScaler()),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fef59229a6e1ff5434759995be53a28a25493efd6d26eb6c004eefe62e31083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
